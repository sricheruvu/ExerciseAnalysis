---
title: "PracticalML-Project"
author: "Sriram Cheruvu"
date: "July 17, 2018"
output: html_document
---

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set("knitr",echo=TRUE, warning=FALSE, message=FALSE, cache=FALSE, fig.width=10, fig.height=5)
options(width=120)

library(caret)
library(rpart)
library(rattle)
library(scales)
library(randomForest)
library(corrplot)
library(lattice)
library(plyr)
library(ggplot2)
library(cluster)
library(Rmisc)
set.seed(1337)

setwd("~/GitHub/PracticalML-Project")
```

### Executive Summary
Based on a dataset provide by HAR [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har) we will try to predict the manner in which the user did the exercise. The training data set contains the target variable classe, all other variables will be used to predict for it.

The following key steps are performed:

1) Load, Process and Clean the data- I started by cleaning the dataset, removing columns that were not related and readings that were NA."" or DIV/0 values. 
2) Explore the data, especially focussing on the paramaters we are interested in 
3) Machine Learning model selection, where we try different models to help us answer our questions
4) A Conclusion where we answer the questions based on the data
5) Predicting the classification of the model on test set
   
For the ML I would be trying 2 models:

a) Fast recursive partitioning model (rpart) and compare this to 
b) Random Forest model.

### 1) Load, Process and Clean the data
```{r loadData}
train_dataURL = 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
test_dataURL = 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'

if (file.exists('pml-training.csv') == FALSE) {
  download.file(train_dataURL, 'pml-training.csv')
}
if (file.exists('pml-testing.csv') == FALSE) {
  download.file(test_dataURL, 'pml-testing.csv')
}
```
Next, read the data into the data frame.
```{r cleanData1}
pmlTrainingData <- read.csv('pml-training.csv', na.strings=c("","NA","!DIV/0"))
finalTest <- read.csv('pml-testing.csv', na.strings=c("","NA","!DIV/0"))
```

### 2) Exploratory Data Analysis
Now we are ready to partition the training and test datasets.Once the partition is ready we will study the correlations of variables with classe.

#### Create Training & Cross Validation Datasets
The full training dataset it split into a training dataset and a testing 
dataset. The testing data will be used to cross validate our models.
```{r createPartition}
inTrain <- createDataPartition(pmlTrainingData$classe, p=.7, list=FALSE)
training <- pmlTrainingData[inTrain,]
testing <- pmlTrainingData[-inTrain,]

summary(training$classe)
```

#### Clean Data
Next, time-related & recording variables and the row index variable X are 
removed because the purpose of the ML assignment is to make predictions.
```{r createDatasets}
training <- training[, -c(1:7)]
testing <- testing[, -c(1:7)]
finalTest <- finalTest[, -c(1:7)]
```
First, I removed variables which contained a majority of missing values. NAs and
blank fields were both marked as NA when the CSV was read.
```{r cleanData2}
removeNAs <- which(colSums(is.na(training)) > nrow(training)/2)
training <- training[, -removeNAs]
testing <- testing[, -removeNAs]
finalTest <- finalTest[, -removeNAs]
```
#### Study Correlations
What are some fields that have high correlations with the classe?
```{r correlations}
classeIndex <- which(names(training) == "classe")
correlations <- cor(training[, -classeIndex], as.numeric(training$classe))
bestCorrelations <- subset(as.data.frame(as.table(correlations)), abs(Freq)>0.3)
bestCorrelations

correlationMatrix <- cor(training[, -classeIndex])
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.9, exact=TRUE)
excludeColumns <- c(highlyCorrelated, classeIndex)
corrplot(correlationMatrix, method="color", type="lower", order="hclust", tl.cex=0.70, tl.col="black", tl.srt = 45, diag = FALSE)

```

We see that there are some features that are quite correlated with each other. We will have a model with these excluded. Even the best correlations with classe are under 0.35. Let's check visually if there is indeed hard to use these 2 as possible simple linear predictors.
```{r echo=FALSE}
p1 <- ggplot(training, aes(classe,pitch_forearm)) + 
  geom_boxplot(aes(fill=classe))

p2 <- ggplot(training, aes(classe, magnet_arm_x)) + 
  geom_boxplot(aes(fill=classe))

multiplot(p1,p2,cols=2)
```

## 3) Machine Learning
First, lets start with a fast recursive partitioning model (rpart) to 
start to see if that would produce reasonable predictions.  

### Recursive partitioning Model
Starting with a simple model. Train the decision tree model
```{r recursivePartModel}
rpModelFit <- train(classe ~ ., method="rpart", data=training, model=TRUE)
rpModelFit$finalModel
```
Next, plot the model
```{r plotRecursivePartModel}
fancyRpartPlot(rpModelFit$finalModel, sub='')
```
Predict `classe` for cross validation dataset
```{r rPathPrediction}
rpPreds <- predict(rpModelFit, newdata=testing)
rpConMatrix <- confusionMatrix(rpPreds, testing$classe)
rpConMatrix
```
We observe a low accuracy with Recursive partitioning model. 
```{r rPathAccuracy}
rpAccuracy = rpConMatrix$overall[[1]]
percent(rpAccuracy)
```
The estimated out of sample error with the cross validation dataset for this 
model is
```{r sampleErrorRpath}
percent(1.00-rpAccuracy)
```
Unfortunately the estimated out of sample error for the `rpart` model was 51% and too high.

### Random Forest Model

Next lets try a random forest model. Using this model, 20 predictions will be made for the test data set.I believe, Random Forests should be a better learning model for our dataset.
```{r RandomForest}
fitControl <- trainControl(method="cv", number=3, verboseIter=F)
rfModelFit <- train(classe ~., method="rf", data=training, trControl=fitControl)
rfModelFit$finalModel
```

Predict `classe` for cross validation dataset.
```{r randomForestPredict}
rfPredictions <- predict(rfModelFit, newdata=testing)
rfConMatrix <- confusionMatrix(rfPredictions, testing$classe)
rfConMatrix
```
We notice a much higher accuracy with a Random Forest Model. This model
performed really well with an estimated out of sample of only 0.7%.
```{r randomForestAccuracy}
rfAccuracy = rfConMatrix$overall[[1]]
percent(rfAccuracy)
```
The estimated out of sample error with the cross validation dataset for this 
model is
```{r randomForestSampleError}
percent(1.00-rfAccuracy)
```

### 4) Conclusion
The Random Forest model outperformed the the Recursive partitioning model by 
quite a bit.  The random forest model was selected for final submissions of the 
project.

### 5) Test Data Simulation
The random forest clearly performs better, approaching 99% accuracy for in-sample and out-of-sample error so we will select this model and apply it to the test data set. We use the provided function to classify 20 data points from the test set by the type of lift.
```{r testSimulation}
submissionPredictions <- predict(rfModelFit, newdata=finalTest)
submissionPredictions
```

